{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you will learn today**: This lab serves as an introduction to PyTorch. We will learn the different steps required in training a deep learning model with modern libraries, such as PyTorch. The idea is to make the model consistent with the structure of previous labs. In other words, we will simply call the **fit** function (like Scikit-learn!) and the model will train. \n",
    "\n",
    "So, which are these steps?\n",
    "\n",
    "* Preliminaries:\n",
    "    * load the train and test datasets, `train_dataset` and `test_dataset` (MNIST in our case)\n",
    "    * turn the datasets into a \"dataloaders\": `train_dataloader` and `test_dataloader`\n",
    "    * define your `model` architecture\n",
    "    * define your `optimizer`, e.g. SGD\n",
    "\n",
    "\n",
    "* Training: Now we have all the building blocks and we need to make our model \"learn\". In most cases, the training follows a specific \"recipe\". Specifically, we feed the `model` the whole `train_dataset` using batches that come from the `train_dataloader`. We repeat this a certain number of times, called `epochs`. Each epoch consists of `batches`. So what do we do for each batch?\n",
    "    * zero out the optimizer. In essence we prepare the optimizer for the incoming data\n",
    "    * compute the output of the model $f(\\cdot)$ for our current data: $x\\mapsto f(x)$\n",
    "    * compute the loss: $\\mathcal{L}(f(x), y)$ where $y$ denotes the ground truth\n",
    "    * perform the `backpropagation` algorithm which involves computing the gradients and performing the update rule\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the preliminaries out of the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we load all the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the datasets. We are going to work with MNIST and our goal is classify digits. This is a popular dataset and PyTorch offers it out-of-the-box, making our life easy! We simply need to call the corresponding method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data are given as PIL images. We need to convert our data to a type \n",
    "# that is readable by a Neural Network. Thus, we use the ToTensor() \"transform\" \n",
    "transform = transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    # torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# load the train dataset\n",
    "train_dataset =  # INSERT YOUR CODE HERE\n",
    "\n",
    "# load the test dataset\n",
    "test_dataset = # INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyperparameters\n",
    "BATCH_SIZE = 1024\n",
    "TEST_BATCH_SIZE = 2048\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# find out which device is available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we cannot use the whole dataset; it is too large for computers to handle. Instead, we perform *stochastic* gradient descent, i.e. we feed the model part of the data called batches. In order to do so, we use Pytorch DataLoaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the dataloader for the traininig dataset. \n",
    "# Here we shuffle the data to promote stochasticity.\n",
    "# The dataloader \n",
    "train_dataloader = # INSERT YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Construct the dataloader for the testing dataset.\n",
    "test_dataloader = # INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "images = next(iter(train_dataloader))[0][:10]\n",
    "grid = torchvision.utils.make_grid(images, nrow=5, padding=10)\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to define our model. We will start with a simple model, a MultiLayer Perceptron (MLP) with 2 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # define the different modules of the network\n",
    "        super(Net, self).__init__()\n",
    "        # input layer has 28x28=784 features \n",
    "        self.fc1 = nn.Linear(784, 50)\n",
    "        # the output layer has 10 neurons. i.e. the number of output classes.\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        # we also define the non-linearity \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # You should (a) transform the a size that is readable\n",
    "        # by the MLP and (b) pass the the input x successively \n",
    "        # through the layers.\n",
    "        # ***************************************************s\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = Net()\n",
    "\n",
    "# move model to device\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define:\n",
    "* the `fit` function that performs the training part\n",
    "* the `predict` function that takes as input the test dataloader and prints the performance metrics (e.g. accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_dataloader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            # move data and target to the device. \n",
    "            # Model and data must be on the same device\n",
    "            # INSERT YOUR CODE HERE\n",
    "\n",
    "            # do the forward pass\n",
    "            output = # INSERT YOUR CODE HERE\n",
    "\n",
    "            # compute the loss\n",
    "            loss = # INSERT YOUR CODE HERE\n",
    "\n",
    "            # compute the running test loss\n",
    "            test_loss += loss.item()\n",
    "            # ... and how many samples were correctly classified\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    accuracy = 100. * correct / len(test_dataloader.dataset)\n",
    "\n",
    "    print(f'Test set: Avg. loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_dataloader.dataset)} ({accuracy:.0f}%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a \"sanity check\". Our model is at the moment initialized randomly and we have 10 classes (each class has approximately the same number of samples). This means that we should get random performance -> ~10% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model=model, test_dataloader=test_dataloader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dataloader, optimizer, device=None):\n",
    "    '''\n",
    "    This function implements the core components of any Neural Network training regiment.\n",
    "    In our stochastic setting our code follows a very specific \"path\". First, we load the batch\n",
    "    a single batch and zero the optimizer. Then we perform the forward pass, compute the gradients and perform the backward pass. And ...repeat!\n",
    "    '''\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        # move data and target to device\n",
    "        # INSERT YOUR CODE HERE\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        # INSERT YOUR CODE HERE\n",
    "\n",
    "        # do the forward pass\n",
    "        output = # INSERT YOUR CODE HERE\n",
    "\n",
    "        # compute the loss\n",
    "        loss = # INSERT YOUR CODE HERE\n",
    "\n",
    "        # compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # perform the backpropagation step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(train_dataloader.dataset)\n",
    "\n",
    "\n",
    "def fit(model, train_dataloader, optimizer, epochs, device):\n",
    "    '''\n",
    "    the fit method simply calls the train_epoch() method for a \n",
    "    specified number of epochs.\n",
    "    '''\n",
    "\n",
    "    # keep track of the losses in order to visualize them later\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = train_epoch(\n",
    "            model=model, \n",
    "            train_dataloader=train_dataloader, \n",
    "            optimizer=optimizer, \n",
    "            # device=device\n",
    "        )\n",
    "        print(f\"Epoch {epoch}: Loss={running_loss}\")\n",
    "        losses.append(running_loss)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = fit(\n",
    "    model=model, \n",
    "    train_dataloader=train_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    epochs=10,\n",
    "    device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the loss progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss progression across epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model=model, test_dataloader=test_dataloader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not very good. There are some major problems. We see from the plot above that the loss keeps dropping and does not \"plateau\". This indicates that we can run the optimization a few more epochs and improve the performance. Another point is that our learning rate is too sloww or the selection of vanilla SGD as our optimizer is not optimal. In the next section we will see that simply changing the optimizer (from SGD to Adam) yields very different results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have created a model, its optimizer, functions for training and predicting. Our end goal is to have something more \"object oriented\". In other words, we want a model with the ease and clarity of scikit-learn: simply call model.fit() and training runs.\n",
    "\n",
    "We throw everything we have written above into a single class (+some extra functionality). After all, every model has a different forward function while the remaining structure stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BasicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Here we define the model modules\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # defines the forward function of the model. \n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def fit(self, train_dataloader, optimizer, epochs, device, plot_loss=True):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = self.train_epoch(\n",
    "                train_dataloader=train_dataloader, \n",
    "                optimizer=optimizer, \n",
    "                epoch_idx=epoch,\n",
    "                device=device)\n",
    "            \n",
    "            losses.append(running_loss)\n",
    "\n",
    "        if plot_loss:\n",
    "            self.plot_loss_progression(losses=losses)\n",
    "\n",
    "    def plot_loss_progression(self, losses):\n",
    "        plt.plot(losses)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss progression across epochs\")\n",
    "\n",
    "    def train_epoch(self, train_dataloader, optimizer, epoch_idx, device):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        self.train()\n",
    "        tk0 = tqdm(train_dataloader, total=len(train_dataloader), desc=f\"Epoch {epoch_idx}\")\n",
    "        for batch_idx, (data, target) in enumerate(tk0):\n",
    "            \n",
    "            # ***************************************************\n",
    "            # INSERT YOUR CODE HERE\n",
    "            # Copy paste from before.\n",
    "            # ***************************************************   \n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            avg_loss = running_loss / (batch_idx + 1)\n",
    "            tk0.set_postfix(loss=avg_loss, stage=\"train\")\n",
    "\n",
    "        \n",
    "        return running_loss / len(train_dataloader.dataset)\n",
    "\n",
    "\n",
    "    def predict(self, test_dataloader, device):\n",
    "        self.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_dataloader:\n",
    "                # ***************************************************\n",
    "                # INSERT YOUR CODE HERE\n",
    "                # Copy paste from before.\n",
    "                # ***************************************************   \n",
    "                test_loss += loss.item()\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "        test_loss /= len(test_dataloader.dataset)\n",
    "        accuracy = 100. * correct / len(test_dataloader.dataset)\n",
    "\n",
    "        print(f'Test set: Avg. loss: {test_loss:.4f}, Accuracy: {correct}/{len(train_dataloader.dataset)} ({accuracy:.0f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the class above is not actually a model, but it defines all the relevant functions. Now we create the model by simply **inheriting** the class above and defining two `__init__` and the `forward` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a model identical with the previous one.\n",
    "\n",
    "class MLP(BasicModel): # inherit the BasicModel class\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input layer has 28x28=784 features \n",
    "        self.fc1 = nn.Linear(784, 50)\n",
    "        # the output layer has 10 neurons. i.e. the number of output classes.\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        # we also define the non-linearity \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # Copy paste from before.\n",
    "        # ***************************************************   \n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model and define the optimizer. \n",
    "mlp = MLP().to(DEVICE)\n",
    "\n",
    "# Instead of SGD we will use a more sophisticated one called Adam.\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# train the mlp\n",
    "mlp.fit(\n",
    "    train_dataloader=train_dataloader, \n",
    "    optimizer=optimizer,\n",
    "    epochs=10,\n",
    "    device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.predict(test_dataloader=test_dataloader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are much better using Adam (in the same number of epochs). This shows the importance of selecting the correct optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the MLP does not take into account the nature of images: close pixels convey local information that is important. Using an MLP, we do not have the notion of the \"pixel neighbourhood\". We, therefore, neglect important information with an MLP. There are however models better suited for vision problems, such as Convolutional Neural Networks or CNNs.\n",
    "\n",
    "With the code structure we have created, we can simply define a CNN and test its performance quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(BasicModel): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # We use a Sequential, i.e. the inputs passes through each of\n",
    "        # the modules below, one-by-one\n",
    "        self.conv = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,                  \n",
    "            ),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2), \n",
    "            nn.Conv2d(\n",
    "                in_channels=16, \n",
    "                out_channels=32, \n",
    "                kernel_size=5, \n",
    "                stride=1, \n",
    "                padding=2),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),    \n",
    "        )\n",
    "              \n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # Copy paste from before.\n",
    "        # *************************************************** \n",
    "        return x   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model and define the optimizer. Instead of SGD we will use a more sophisticate one called Adam.\n",
    "cnn = CNN().to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# train the mlp\n",
    "cnn.fit(\n",
    "    train_dataloader=train_dataloader, \n",
    "    optimizer=optimizer,\n",
    "    epochs=10,\n",
    "    device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.predict(test_dataloader=test_dataloader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN outperforms the MLP, which is to be expected in out case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: play with CIFAR10\n",
    "\n",
    "MNIST is a fairly simple dataset. What happens in more challenging datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
